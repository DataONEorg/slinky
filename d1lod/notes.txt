- Add a Job class w/ a perform method, logging stuff, etc
- Do we need Python dateutil?
- Re-visit logging
- f-string everything
- consider a rename of Graph and Interface
- factor out constants better
- deal with env vars in a 12factor way
- re-do job queues
- httpx instead of requests
- do we still do XML with ElementTree?
- f-strings everywhere?

core classes

- dataone interface???
  - main area for logic about rdf patterns
  - get new objects since x datetime
  - get sysmeta
  - get object bytes
  - manages an in-memory RDF model which can be sent to sparql api interface
- sparql api interface
  - knows about sparql api endpoints and any details
  - generates sparql queries
  - sends them
  - handles response
- metadata processors
  - sysmeta, eml, iso, etc
  - convert file -> triples


SlinkyClient
  # Handles local RDF model
  processDataset(id)
TripleStore
  # Handles queries to triplestore
  instance_variables
    host
    port
    path
    session # shared http session
  methods
    createGraph
    deleteGraph
    insertTriple?
    insertStatement?
    insertModel
    sendRequest
    handleResponse
    listGraphs
    getSize
    statementExists
  subclasses
    SparqlTripleStore # for SPARQL
GraphPattern
  DataONEPattern

job gets processDataset(dataset_pid)
  - Creates a SlinkClient
  - Calls processDataset(dataset_id)
    - Creates a one-off RDF.Model
    - Fetches data from DataONE and triplifies it into the model
    - Inserts it into the graph
